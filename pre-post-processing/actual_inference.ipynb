{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgN1cLL8F5iB"
   },
   "outputs": [],
   "source": [
    "#run in sequence\n",
    "#this runs the evaluation process at one checkpoint (run after running frozen_graph.ipynb)\n",
    "#colab limits outputs to last 5000 lines, so manually input the checkpoint number to not miss anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ihv0PTJFYfKS"
   },
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "(for colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "b_Tim6tZOqfg",
    "outputId": "2ba91bb8-0fb0-4fbe-fa90-175827e937cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hdi9-TKZYlKM"
   },
   "source": [
    "# Clone Tensorflow Models directory from Github, install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "colab_type": "code",
    "id": "Vc_OhY3VP9NU",
    "outputId": "b7162474-d555-4f44-d181-f3b6362a5095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 33869, done.\u001b[K\n",
      "remote: Total 33869 (delta 0), reused 0 (delta 0), pack-reused 33869\n",
      "Receiving objects: 100% (33869/33869), 512.28 MiB | 29.84 MiB/s, done.\n",
      "Resolving deltas: 100% (21833/21833), done.\n",
      "Checking out files: 100% (2491/2491), done.\n",
      "Collecting pascal-voc-writer\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/82/dd86999e6062fc34478f11ead7a68e6615d7e270b39624547edd1dbaba76/pascal_voc_writer-0.1.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from pascal-voc-writer) (2.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->pascal-voc-writer) (1.1.1)\n",
      "Installing collected packages: pascal-voc-writer\n",
      "Successfully installed pascal-voc-writer-0.1.4\n",
      "Selecting previously unselected package python-bs4.\n",
      "(Reading database ... 144568 files and directories currently installed.)\n",
      "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
      "Unpacking python-bs4 (4.6.0-1) ...\n",
      "Selecting previously unselected package python-pkg-resources.\n",
      "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
      "Unpacking python-pkg-resources (39.0.1-2) ...\n",
      "Selecting previously unselected package python-chardet.\n",
      "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
      "Unpacking python-chardet (3.0.4-1) ...\n",
      "Selecting previously unselected package python-six.\n",
      "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
      "Unpacking python-six (1.11.0-2) ...\n",
      "Selecting previously unselected package python-webencodings.\n",
      "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
      "Unpacking python-webencodings (0.5-2) ...\n",
      "Selecting previously unselected package python-html5lib.\n",
      "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
      "Unpacking python-html5lib (0.999999999-1) ...\n",
      "Selecting previously unselected package python-lxml:amd64.\n",
      "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
      "Selecting previously unselected package python-olefile.\n",
      "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
      "Unpacking python-olefile (0.45.1-1) ...\n",
      "Selecting previously unselected package python-pil:amd64.\n",
      "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
      "Setting up python-pkg-resources (39.0.1-2) ...\n",
      "Setting up python-six (1.11.0-2) ...\n",
      "Setting up python-bs4 (4.6.0-1) ...\n",
      "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
      "Setting up python-olefile (0.45.1-1) ...\n",
      "Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
      "Setting up python-webencodings (0.5-2) ...\n",
      "Setting up python-chardet (3.0.4-1) ...\n",
      "Setting up python-html5lib (0.999999999-1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tensorflow/models.git\n",
    "!apt-get -qq install libprotobuf-java protobuf-compiler\n",
    "!pip install pascal-voc-writer\n",
    "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
    "!pip install -qq Cython contextlib2 pillow lxml matplotlib pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoPBeEpeYptF"
   },
   "source": [
    "# Build and Install setup.py in the models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3Tfpx41wQEry",
    "outputId": "2ec559c8-a0ec-439d-8404-8c3b44895e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating object_detection.egg-info\n",
      "writing object_detection.egg-info/PKG-INFO\n",
      "writing dependency_links to object_detection.egg-info/dependency_links.txt\n",
      "writing requirements to object_detection.egg-info/requires.txt\n",
      "writing top-level names to object_detection.egg-info/top_level.txt\n",
      "writing manifest file 'object_detection.egg-info/SOURCES.txt'\n",
      "reading manifest file 'object_detection.egg-info/SOURCES.txt'\n",
      "writing manifest file 'object_detection.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
      "\n",
      "creating build\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying object_detection.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying object_detection.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying object_detection.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying object_detection.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying object_detection.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating dist\n",
      "creating 'dist/object_detection-0.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing object_detection-0.1-py3.6.egg\n",
      "Copying object_detection-0.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
      "Adding object-detection 0.1 to easy-install.pth file\n",
      "\n",
      "Installed /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg\n",
      "Processing dependencies for object-detection==0.1\n",
      "Searching for Cython==0.29.16\n",
      "Best match: Cython 0.29.16\n",
      "Adding Cython 0.29.16 to easy-install.pth file\n",
      "Installing cygdb script to /usr/local/bin\n",
      "Installing cython script to /usr/local/bin\n",
      "Installing cythonize script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for matplotlib==3.2.1\n",
      "Best match: matplotlib 3.2.1\n",
      "Adding matplotlib 3.2.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for Pillow==7.0.0\n",
      "Best match: Pillow 7.0.0\n",
      "Adding Pillow 7.0.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for pyparsing==2.4.7\n",
      "Best match: pyparsing 2.4.7\n",
      "Adding pyparsing 2.4.7 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for numpy==1.18.2\n",
      "Best match: numpy 1.18.2\n",
      "Adding numpy 1.18.2 to easy-install.pth file\n",
      "Installing f2py script to /usr/local/bin\n",
      "Installing f2py3 script to /usr/local/bin\n",
      "Installing f2py3.6 script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for cycler==0.10.0\n",
      "Best match: cycler 0.10.0\n",
      "Adding cycler 0.10.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for python-dateutil==2.8.1\n",
      "Best match: python-dateutil 2.8.1\n",
      "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for kiwisolver==1.2.0\n",
      "Best match: kiwisolver 1.2.0\n",
      "Adding kiwisolver 1.2.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for six==1.12.0\n",
      "Best match: six 1.12.0\n",
      "Adding six 1.12.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Finished processing dependencies for object-detection==0.1\n"
     ]
    }
   ],
   "source": [
    "!python3 /content/models/research/setup.py build\n",
    "!python3 /content/models/research/setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ooRtkc-LYrQ-"
   },
   "source": [
    "# clone COCO API for Python, make, and copy to models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xOJ2cu0bRYNB",
    "outputId": "91e5039a-ffd8-4900-a0c3-926cc1d5c76e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cocoapi'...\n",
      "remote: Enumerating objects: 975, done.\u001b[K\n",
      "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
      "Receiving objects: 100% (975/975), 11.72 MiB | 12.74 MiB/s, done.\n",
      "Resolving deltas: 100% (576/576), done.\n",
      "python setup.py build_ext --inplace\n",
      "running build_ext\n",
      "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
      "/usr/local/lib/python3.6/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "building 'pycocotools._mask' extension\n",
      "creating build\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-3.6\n",
      "creating build/temp.linux-x86_64-3.6/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I../common -I/usr/include/python3.6m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.6/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
      "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
      "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
      "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
      "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
      "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
      "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
      "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
      "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
      "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I../common -I/usr/include/python3.6m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-3.6\n",
      "creating build/lib.linux-x86_64-3.6/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/../common/maskApi.o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -o build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so -> pycocotools\n",
      "rm -rf build\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cocodataset/cocoapi.git\n",
    "!cd cocoapi/PythonAPI; make; cp -r pycocotools /content/models/research/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Yg9bmMhY0Fu"
   },
   "source": [
    "# build necessary .proto files\n",
    "\n",
    "these help to build pipeline (from pipeline.config), construct anchors and bounding boxes, among other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHIVNYdQRc9q"
   },
   "outputs": [],
   "source": [
    "!cd /content/models/research;protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJvUKD4jY6Ia"
   },
   "source": [
    "# set path for imports to work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rrUNM0xcSLFu",
    "outputId": "42bdd0cd-f9ba-4891-d820-13828b6ce2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tensorflow-1.15.2/python3.6:/env/python/:/content/models/research:/content/models/research/slim:/content/models/research/object_detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONPATH'] = '/tensorflow-1.15.2/python3.6:/env/python/:/content/models/research:/content/models/research/slim:/content/models/research/object_detection'\n",
    "print(os.environ['PYTHONPATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHZCRSxOY83e"
   },
   "source": [
    "# check that object_detection API build successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "colab_type": "code",
    "id": "LjfsPnIRRgOV",
    "outputId": "93cc8734-6ea5-436a-cfb8-920b2fcc8a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Running tests under Python 3.6.9: /usr/bin/python3\n",
      "[ RUN      ] ModelBuilderTest.test_create_experimental_model\n",
      "[       OK ] ModelBuilderTest.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
      "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
      "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
      "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
      "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
      "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
      "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
      "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTest.test_session\n",
      "[  SKIPPED ] ModelBuilderTest.test_session\n",
      "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
      "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
      "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
      "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 17 tests in 0.382s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python3 /content/models/research/object_detection/builders/model_builder_test.py\n",
    "\n",
    "#output at the end should be OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2c71L7QnZBoE"
   },
   "source": [
    "# conducts evaluation on either test or val (check pipeline.config)\n",
    "\n",
    "if you defined your own model architecture, run the cells below this cell first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bXTf8uYT21pv",
    "outputId": "c5d131c2-3897-420e-c499-eda7015195ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=14.16s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.37s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.562\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.902\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.639\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.226\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.598\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.517\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.618\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.649\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.656\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.362\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.685\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/model_main.py:109: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0414 06:04:31.746906 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:628: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n",
      "W0414 06:04:31.751031 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:628: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n",
      "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
      "W0414 06:04:31.751177 140255665346432 model_lib.py:629] Forced number of epochs for all eval validations to be 1.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W0414 06:04:31.751369 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\n",
      "I0414 06:04:31.751491 140255665346432 config_util.py:488] Maybe overwriting train_steps: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0414 06:04:31.751667 140255665346432 config_util.py:488] Maybe overwriting use_bfloat16: False\n",
      "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
      "I0414 06:04:31.751865 140255665346432 config_util.py:488] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
      "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
      "I0414 06:04:31.751997 140255665346432 config_util.py:488] Maybe overwriting eval_num_epochs: 1\n",
      "INFO:tensorflow:Maybe overwriting load_pretrained: True\n",
      "I0414 06:04:31.752136 140255665346432 config_util.py:488] Maybe overwriting load_pretrained: True\n",
      "INFO:tensorflow:Ignoring config override key: load_pretrained\n",
      "I0414 06:04:31.752251 140255665346432 config_util.py:498] Ignoring config override key: load_pretrained\n",
      "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "W0414 06:04:31.753028 140255665346432 model_lib.py:645] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
      "I0414 06:04:31.753182 140255665346432 model_lib.py:680] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'eval_test/50000_eval', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f68ee7860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0414 06:04:31.753581 140255665346432 estimator.py:212] Using config: {'_model_dir': 'eval_test/50000_eval', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f68ee7860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f8f68f4a620>) includes params argument, but params are not passed to Estimator.\n",
      "W0414 06:04:31.753828 140255665346432 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f8f68f4a620>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "W0414 06:04:31.765502 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
      "\n",
      "W0414 06:04:31.765826 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
      "\n",
      "W0414 06:04:31.784632 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.parallel_interleave(...)`.\n",
      "W0414 06:04:31.792179 140255665346432 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.parallel_interleave(...)`.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W0414 06:04:31.792356 140255665346432 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0414 06:04:31.814333 140255665346432 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:Entity <function build.<locals>.process_fn at 0x7f8f68f4aa60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "W0414 06:04:31.833642 140255665346432 ag_logging.py:146] Entity <function build.<locals>.process_fn at 0x7f8f68f4aa60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:491: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n",
      "W0414 06:04:32.009332 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/utils/ops.py:491: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0414 06:04:32.013879 140255665346432 deprecation.py:323] From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:2689: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "W0414 06:04:32.068794 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/core/preprocessor.py:2689: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0414 06:04:32.162893 140255665346432 deprecation.py:323] From /content/models/research/object_detection/inputs.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:454: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0414 06:04:32.536974 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/inputs.py:454: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:470: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.\n",
      "\n",
      "W0414 06:04:32.545385 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/inputs.py:470: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
      "W0414 06:04:32.564768 140255665346432 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I0414 06:04:32.580379 140255665346432 estimator.py:1148] Calling model_fn.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:168: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0414 06:04:32.602797 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:168: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:32.603170 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:32.603331 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0414 06:04:32.604196 140255665346432 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
      "\n",
      "W0414 06:04:42.736831 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
      "\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:42.745207 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:558: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "W0414 06:04:42.745576 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:558: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:42.810286 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0414 06:04:42.810751 140255665346432 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/utils/spatial_transform_ops.py:419: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "W0414 06:04:43.702386 140255665346432 deprecation.py:506] From /content/models/research/object_detection/utils/spatial_transform_ops.py:419: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:191: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W0414 06:04:43.718496 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:191: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:43.718862 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:43.719001 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0414 06:04:45.337758 140255665346432 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:45.340293 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:45.359888 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "W0414 06:04:45.898063 140255665346432 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:45.982100 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I0414 06:04:45.982240 140255665346432 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0414 06:04:46.791026 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/core/minibatch_sampler.py:85: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "W0414 06:04:46.839631 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/core/minibatch_sampler.py:85: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n",
      "W0414 06:04:46.889185 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "W0414 06:04:46.890766 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:350: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W0414 06:04:46.942149 140255665346432 deprecation.py:323] From /content/models/research/object_detection/core/losses.py:350: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2710: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0414 06:04:47.207375 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2710: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2710: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0414 06:04:47.207629 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2710: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:380: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "W0414 06:04:47.208482 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:380: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/builders/optimizer_builder.py:58: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "W0414 06:04:47.216969 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/builders/optimizer_builder.py:58: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:796: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0414 06:04:47.229400 140255665346432 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:796: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:498: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W0414 06:04:47.464952 140255665346432 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:498: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:1044: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0414 06:04:47.646012 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/utils/visualization_utils.py:1044: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:484: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
      "\n",
      "W0414 06:04:47.743827 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:484: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:515: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0414 06:04:47.811816 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:515: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:519: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W0414 06:04:48.613294 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:519: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:520: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "W0414 06:04:48.613588 140255665346432 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:520: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I0414 06:04:48.614508 140255665346432 estimator.py:1150] Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-04-14T06:04:48Z\n",
      "I0414 06:04:48.635615 140255665346432 evaluation.py:255] Starting evaluation at 2020-04-14T06:04:48Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0414 06:04:50.136408 140255665346432 monitored_session.py:240] Graph was finalized.\n",
      "2020-04-14 06:04:50.142048: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2020-04-14 06:04:50.142264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x313b2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-04-14 06:04:50.142287: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-04-14 06:04:50.144568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-04-14 06:04:50.259969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-14 06:04:50.261054: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x313af40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-04-14 06:04:50.261090: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2020-04-14 06:04:50.261285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-14 06:04:50.262021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-04-14 06:04:50.262426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-14 06:04:50.264442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-04-14 06:04:50.266093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-04-14 06:04:50.266473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-04-14 06:04:50.268530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-04-14 06:04:50.269281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-04-14 06:04:50.273510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-04-14 06:04:50.273699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-14 06:04:50.274744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-14 06:04:50.275538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
      "2020-04-14 06:04:50.275644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-14 06:04:50.277080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-04-14 06:04:50.277108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
      "2020-04-14 06:04:50.277121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
      "2020-04-14 06:04:50.277267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-14 06:04:50.278059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-14 06:04:50.278802: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-04-14 06:04:50.278883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "INFO:tensorflow:Restoring parameters from frozen/inference-50000/model.ckpt\n",
      "I0414 06:04:50.281662 140255665346432 saver.py:1284] Restoring parameters from frozen/inference-50000/model.ckpt\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0414 06:04:53.913372 140255665346432 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0414 06:04:54.227967 140255665346432 session_manager.py:502] Done running local_init_op.\n",
      "2020-04-14 06:04:59.735054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-04-14 06:05:01.507806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "INFO:tensorflow:Performing evaluation on 2004 images.\n",
      "I0414 06:18:24.847938 140253614573312 coco_evaluation.py:205] Performing evaluation on 2004 images.\n",
      "INFO:tensorflow:Loading and preparing annotation results...\n",
      "I0414 06:18:24.852685 140253614573312 coco_tools.py:115] Loading and preparing annotation results...\n",
      "INFO:tensorflow:DONE (t=0.53s)\n",
      "I0414 06:18:25.383239 140253614573312 coco_tools.py:137] DONE (t=0.53s)\n",
      "INFO:tensorflow:Finished evaluation at 2020-04-14-06:18:44\n",
      "I0414 06:18:44.965770 140255665346432 evaluation.py:275] Finished evaluation at 2020-04-14-06:18:44\n",
      "INFO:tensorflow:Saving dict for global step 50000: DetectionBoxes_Precision/mAP = 0.56215584, DetectionBoxes_Precision/mAP (large) = 0.51741594, DetectionBoxes_Precision/mAP (medium) = 0.5976074, DetectionBoxes_Precision/mAP (small) = 0.22582442, DetectionBoxes_Precision/mAP@.50IOU = 0.90209794, DetectionBoxes_Precision/mAP@.75IOU = 0.63910145, DetectionBoxes_Recall/AR@1 = 0.6175613, DetectionBoxes_Recall/AR@10 = 0.648807, DetectionBoxes_Recall/AR@100 = 0.65588695, DetectionBoxes_Recall/AR@100 (large) = 0.6438095, DetectionBoxes_Recall/AR@100 (medium) = 0.685491, DetectionBoxes_Recall/AR@100 (small) = 0.362468, Loss/BoxClassifierLoss/classification_loss = 0.108954936, Loss/BoxClassifierLoss/localization_loss = 0.06591462, Loss/RPNLoss/localization_loss = 0.041717865, Loss/RPNLoss/objectness_loss = 0.13548672, Loss/total_loss = 0.35207427, global_step = 50000, learning_rate = 9e-05, loss = 0.35207427\n",
      "I0414 06:18:44.966166 140255665346432 estimator.py:2049] Saving dict for global step 50000: DetectionBoxes_Precision/mAP = 0.56215584, DetectionBoxes_Precision/mAP (large) = 0.51741594, DetectionBoxes_Precision/mAP (medium) = 0.5976074, DetectionBoxes_Precision/mAP (small) = 0.22582442, DetectionBoxes_Precision/mAP@.50IOU = 0.90209794, DetectionBoxes_Precision/mAP@.75IOU = 0.63910145, DetectionBoxes_Recall/AR@1 = 0.6175613, DetectionBoxes_Recall/AR@10 = 0.648807, DetectionBoxes_Recall/AR@100 = 0.65588695, DetectionBoxes_Recall/AR@100 (large) = 0.6438095, DetectionBoxes_Recall/AR@100 (medium) = 0.685491, DetectionBoxes_Recall/AR@100 (small) = 0.362468, Loss/BoxClassifierLoss/classification_loss = 0.108954936, Loss/BoxClassifierLoss/localization_loss = 0.06591462, Loss/RPNLoss/localization_loss = 0.041717865, Loss/RPNLoss/objectness_loss = 0.13548672, Loss/total_loss = 0.35207427, global_step = 50000, learning_rate = 9e-05, loss = 0.35207427\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 50000: frozen/inference-50000/model.ckpt\n",
      "I0414 06:18:47.829038 140255665346432 estimator.py:2109] Saving 'checkpoint_path' summary for global step 50000: frozen/inference-50000/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#cd to model directory\n",
    "cd drive\n",
    "cd 'My Drive'\n",
    "cd model_frcnn_inception_resnet_v2_final\n",
    "#change SPLIT to the checkpoint number\n",
    "SPLIT='50000'\n",
    "#check pipeline.config points to the right tfrecord (test or val)\n",
    "#change model_dir to 'eval' or 'eval_test' as required\n",
    "python3 /content/models/research/object_detection/model_main.py \\\n",
    "  --alsologtostderr \\\n",
    "  --run_once \\\n",
    "  --checkpoint_dir=frozen/inference-${SPLIT} \\\n",
    "  --model_dir=eval_test/${SPLIT}_eval \\\n",
    "  --pipeline_config_path=frozen/inference-${SPLIT}/pipeline.config \n",
    "\n",
    "#after running, scroll up to top of output cell to view metrics (only mAP, mAR, F1 calculated by ourselves)\n",
    "#this will generate folder 'eval' in model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZS0YUIhtKEtu",
    "outputId": "58ba7d63-f1ee-4ed2-ae80-39dc7b2a05e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  faster_rcnn {\n",
      "    num_classes: 3\n",
      "    image_resizer {\n",
      "      keep_aspect_ratio_resizer {\n",
      "        min_dimension: 600\n",
      "        max_dimension: 1024\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"faster_rcnn_inception_resnet_v2\"\n",
      "      first_stage_features_stride: 8\n",
      "    }\n",
      "    first_stage_anchor_generator {\n",
      "      grid_anchor_generator {\n",
      "        height_stride: 8\n",
      "        width_stride: 8\n",
      "        scales: 0.25\n",
      "        scales: 0.5\n",
      "        scales: 1.0\n",
      "        scales: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "      }\n",
      "    }\n",
      "    first_stage_atrous_rate: 2\n",
      "    first_stage_box_predictor_conv_hyperparams {\n",
      "      op: CONV\n",
      "      regularizer {\n",
      "        l2_regularizer {\n",
      "          weight: 0.0\n",
      "        }\n",
      "      }\n",
      "      initializer {\n",
      "        truncated_normal_initializer {\n",
      "          stddev: 0.0099999998\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    first_stage_nms_score_threshold: 0.0\n",
      "    first_stage_nms_iou_threshold: 0.69999999\n",
      "    first_stage_max_proposals: 300\n",
      "    first_stage_localization_loss_weight: 2.0\n",
      "    first_stage_objectness_loss_weight: 1.0\n",
      "    initial_crop_size: 17\n",
      "    maxpool_kernel_size: 1\n",
      "    maxpool_stride: 1\n",
      "    second_stage_box_predictor {\n",
      "      mask_rcnn_box_predictor {\n",
      "        fc_hyperparams {\n",
      "          op: FC\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 0.0\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            variance_scaling_initializer {\n",
      "              factor: 1.0\n",
      "              uniform: true\n",
      "              mode: FAN_AVG\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        use_dropout: false\n",
      "        dropout_keep_probability: 1.0\n",
      "      }\n",
      "    }\n",
      "    second_stage_post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 0.0\n",
      "        iou_threshold: 0.60000002\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 300\n",
      "      }\n",
      "      score_converter: SOFTMAX\n",
      "    }\n",
      "    second_stage_localization_loss_weight: 2.0\n",
      "    second_stage_classification_loss_weight: 1.0\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  batch_size: 1\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "  optimizer {\n",
      "    momentum_optimizer {\n",
      "      learning_rate {\n",
      "        manual_step_learning_rate {\n",
      "          initial_learning_rate: 9.9999997e-05\n",
      "          schedule {\n",
      "            step: 50000\n",
      "            learning_rate: 9.0000001e-05\n",
      "          }\n",
      "          schedule {\n",
      "            step: 100000\n",
      "            learning_rate: 8.0999998e-05\n",
      "          }\n",
      "          schedule {\n",
      "            step: 150000\n",
      "            learning_rate: 7.2900002e-05\n",
      "          }\n",
      "          schedule {\n",
      "            step: 200000\n",
      "            learning_rate: 6.5610002e-05\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.89999998\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  gradient_clipping_by_norm: 10.0\n",
      "  fine_tune_checkpoint: \"/content/drive/My Drive/model_frcnn_inception_resnet_v2_final/model.ckpt\"\n",
      "  from_detection_checkpoint: true\n",
      "  num_steps: 250000\n",
      "}\n",
      "train_input_reader {\n",
      "  label_map_path: \"/content/drive/My Drive/label_map.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"/content/drive/My Drive/xmls/trainxml.record\"\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  num_examples: 2004\n",
      "  max_evals: 2004\n",
      "  use_moving_averages: false\n",
      "}\n",
      "eval_input_reader {\n",
      "  label_map_path: \"/content/drive/My Drive/label_map.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_readers: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"/content/drive/My Drive/xmls/testxml.record\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#you can use the following line to check if pipeline.config is correct\n",
    "#!cat /content/drive/'My Drive'/model_frcnn_inception_resnet_v2_final/frozen/inference-50000/pipeline.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NL5M11gSVI8y"
   },
   "outputs": [],
   "source": [
    "#run the below cells if you defined your own model architecture (for us, only resnet50rpn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N74neoWgZR2O"
   },
   "source": [
    "# only if you defined your own model architecture\n",
    "\n",
    "writes the feature_extractor.py to models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yEz6QIiz0srL",
    "outputId": "51f2a7c9-e351-4c63-9ef7-063682b6567b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/models/research/object_detection/models/faster_rcnn_resnet_v100_feature_extractor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/models/research/object_detection/models/faster_rcnn_resnet_v100_feature_extractor.py\n",
    "\n",
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Resnet V1 Faster R-CNN implementation.\n",
    "\n",
    "See \"Deep Residual Learning for Image Recognition\" by He et al., 2015.\n",
    "https://arxiv.org/abs/1512.03385\n",
    "\n",
    "Note: this implementation assumes that the classification checkpoint used\n",
    "to finetune this model is trained using the same configuration as that of\n",
    "the MSRA provided checkpoints\n",
    "(see https://github.com/KaimingHe/deep-residual-networks), e.g., with\n",
    "same preprocessing, batch norm scaling, etc.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.meta_architectures import faster_rcnn_meta_arch\n",
    "from nets import resnet_utils\n",
    "from nets import resnet_v1\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "class FasterRCNNResnetV100FeatureExtractor(\n",
    "    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet V1 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               architecture,\n",
    "               resnet_model,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      architecture: Architecture name of the Resnet V1 model.\n",
    "      resnet_model: Definition of the Resnet V1 model.\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16.\n",
    "    \"\"\"\n",
    "    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n",
    "      raise ValueError('`first_stage_features_stride` must be 8 or 16.')\n",
    "    self._architecture = architecture\n",
    "    self._resnet_model = resnet_model\n",
    "    super(FasterRCNNResnet50FeatureExtractor, self).__init__(\n",
    "        'resnet_v1_50', resnet_v1.resnet_v1_50, is_training,\n",
    "        first_stage_features_stride, batch_norm_trainable,\n",
    "        reuse_weights, weight_decay)\n",
    "\n",
    "  def preprocess(self, resized_inputs):\n",
    "    \"\"\"Faster R-CNN Resnet V1 preprocessing.\n",
    "\n",
    "    VGG style channel mean subtraction as described here:\n",
    "    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n",
    "    Note that if the number of channels is not equal to 3, the mean subtraction\n",
    "    will be skipped and the original resized_inputs will be returned.\n",
    "\n",
    "    Args:\n",
    "      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n",
    "        representing a batch of images with values between 0 and 255.0.\n",
    "\n",
    "    Returns:\n",
    "      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n",
    "        tensor representing a batch of images.\n",
    "\n",
    "    \"\"\"\n",
    "    if resized_inputs.shape.as_list()[3] == 3:\n",
    "      channel_means = [123.68, 116.779, 103.939]\n",
    "      return resized_inputs - [[channel_means]]\n",
    "    else:\n",
    "      return resized_inputs\n",
    "\n",
    "  def _extract_proposal_features(self, preprocessed_inputs, scope):\n",
    "    \"\"\"Extracts first stage RPN features.\n",
    "\n",
    "    Args:\n",
    "      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n",
    "        representing a batch of images.\n",
    "      scope: A scope name.\n",
    "\n",
    "    Returns:\n",
    "      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n",
    "      activations: A dictionary mapping feature extractor tensor names to\n",
    "        tensors\n",
    "\n",
    "    Raises:\n",
    "      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n",
    "        (height or width) is less than 33.\n",
    "      ValueError: If the created network is missing the required activation.\n",
    "    \"\"\"\n",
    "    if len(preprocessed_inputs.get_shape().as_list()) != 4:\n",
    "      raise ValueError('`preprocessed_inputs` must be 4 dimensional, got a '\n",
    "                       'tensor of shape %s' % preprocessed_inputs.get_shape())\n",
    "    shape_assert = tf.Assert(\n",
    "        tf.logical_and(\n",
    "            tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n",
    "            tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n",
    "        ['image size must at least be 33 in both height and width.'])\n",
    "\n",
    "    with tf.control_dependencies([shape_assert]):\n",
    "      # Disables batchnorm for fine-tuning with smaller batch sizes.\n",
    "      # TODO(chensun): Figure out if it is needed when image\n",
    "      # batch size is bigger.\n",
    "      with slim.arg_scope(\n",
    "          resnet_utils.resnet_arg_scope(\n",
    "              batch_norm_epsilon=1e-5,\n",
    "              batch_norm_scale=True,\n",
    "              weight_decay=self._weight_decay)):\n",
    "        with tf.variable_scope(\n",
    "            self._architecture, reuse=self._reuse_weights) as var_scope:\n",
    "          _, activations = self._resnet_model(\n",
    "              preprocessed_inputs,\n",
    "              num_classes=None,\n",
    "              is_training=self._train_batch_norm,\n",
    "              global_pool=False,\n",
    "              output_stride=self._first_stage_features_stride,\n",
    "              spatial_squeeze=False,\n",
    "              scope=var_scope)\n",
    "\n",
    "    handle = scope + '/%s/block2' % self._architecture\n",
    "    return activations[handle], activations\n",
    "\n",
    "  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
    "    \"\"\"Extracts second stage box classifier features.\n",
    "\n",
    "    Args:\n",
    "      proposal_feature_maps: A 4-D float tensor with shape\n",
    "        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n",
    "        representing the feature map cropped to each proposal.\n",
    "      scope: A scope name (unused).\n",
    "\n",
    "    Returns:\n",
    "      proposal_classifier_features: A 4-D float tensor with shape\n",
    "        [batch_size * self.max_num_proposals, height, width, depth]\n",
    "        representing box classifier features for each proposal.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(self._architecture, reuse=self._reuse_weights):\n",
    "      with slim.arg_scope(\n",
    "          resnet_utils.resnet_arg_scope(\n",
    "              batch_norm_epsilon=1e-5,\n",
    "              batch_norm_scale=True,\n",
    "              weight_decay=self._weight_decay)):\n",
    "        with slim.arg_scope([slim.batch_norm],\n",
    "                            is_training=self._train_batch_norm):\n",
    "          blocks = [\n",
    "              resnet_utils.Block('block4', resnet_v1.bottleneck, [{\n",
    "                  'depth': 2048,\n",
    "                  'depth_bottleneck': 512,\n",
    "                  'stride': 1\n",
    "              }] * 3)\n",
    "          ]\n",
    "          proposal_classifier_features = resnet_utils.stack_blocks_dense(\n",
    "              proposal_feature_maps, blocks)\n",
    "    return proposal_classifier_features\n",
    "\n",
    "class FasterRCNNResnetV1FeatureExtractor(\n",
    "    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet V1 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               architecture,\n",
    "               resnet_model,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0,\n",
    "               activation_fn=tf.nn.relu):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      architecture: Architecture name of the Resnet V1 model.\n",
    "      resnet_model: Definition of the Resnet V1 model.\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "      activation_fn: Activaton functon to use in Resnet V1 model.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16.\n",
    "    \"\"\"\n",
    "    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n",
    "      raise ValueError('`first_stage_features_stride` must be 8 or 16.')\n",
    "    self._architecture = architecture\n",
    "    self._resnet_model = resnet_model\n",
    "    self._activation_fn = activation_fn\n",
    "    super(FasterRCNNResnetV1FeatureExtractor,\n",
    "          self).__init__(is_training, first_stage_features_stride,\n",
    "                         batch_norm_trainable, reuse_weights, weight_decay)\n",
    "\n",
    "  def preprocess(self, resized_inputs):\n",
    "    \"\"\"Faster R-CNN Resnet V1 preprocessing.\n",
    "\n",
    "    VGG style channel mean subtraction as described here:\n",
    "    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n",
    "    Note that if the number of channels is not equal to 3, the mean subtraction\n",
    "    will be skipped and the original resized_inputs will be returned.\n",
    "\n",
    "    Args:\n",
    "      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n",
    "        representing a batch of images with values between 0 and 255.0.\n",
    "\n",
    "    Returns:\n",
    "      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n",
    "        tensor representing a batch of images.\n",
    "\n",
    "    \"\"\"\n",
    "    if resized_inputs.shape.as_list()[3] == 3:\n",
    "      channel_means = [123.68, 116.779, 103.939]\n",
    "      return resized_inputs - [[channel_means]]\n",
    "    else:\n",
    "      return resized_inputs\n",
    "\n",
    "  def _extract_proposal_features(self, preprocessed_inputs, scope):\n",
    "    \"\"\"Extracts first stage RPN features.\n",
    "\n",
    "    Args:\n",
    "      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n",
    "        representing a batch of images.\n",
    "      scope: A scope name.\n",
    "\n",
    "    Returns:\n",
    "      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n",
    "      activations: A dictionary mapping feature extractor tensor names to\n",
    "        tensors\n",
    "\n",
    "    Raises:\n",
    "      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n",
    "        (height or width) is less than 33.\n",
    "      ValueError: If the created network is missing the required activation.\n",
    "    \"\"\"\n",
    "    if len(preprocessed_inputs.get_shape().as_list()) != 4:\n",
    "      raise ValueError('`preprocessed_inputs` must be 4 dimensional, got a '\n",
    "                       'tensor of shape %s' % preprocessed_inputs.get_shape())\n",
    "    shape_assert = tf.Assert(\n",
    "        tf.logical_and(\n",
    "            tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n",
    "            tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n",
    "        ['image size must at least be 33 in both height and width.'])\n",
    "\n",
    "    with tf.control_dependencies([shape_assert]):\n",
    "      # Disables batchnorm for fine-tuning with smaller batch sizes.\n",
    "      # TODO(chensun): Figure out if it is needed when image\n",
    "      # batch size is bigger.\n",
    "      with slim.arg_scope(\n",
    "          resnet_utils.resnet_arg_scope(\n",
    "              batch_norm_epsilon=1e-5,\n",
    "              batch_norm_scale=True,\n",
    "              activation_fn=self._activation_fn,\n",
    "              weight_decay=self._weight_decay)):\n",
    "        with tf.variable_scope(\n",
    "            self._architecture, reuse=self._reuse_weights) as var_scope:\n",
    "          _, activations = self._resnet_model(\n",
    "              preprocessed_inputs,\n",
    "              num_classes=None,\n",
    "              is_training=self._train_batch_norm,\n",
    "              global_pool=False,\n",
    "              output_stride=self._first_stage_features_stride,\n",
    "              spatial_squeeze=False,\n",
    "              scope=var_scope)\n",
    "\n",
    "    handle = scope + '/%s/block2' % self._architecture\n",
    "    return activations[handle], activations\n",
    "\n",
    "  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
    "    \"\"\"Extracts second stage box classifier features.\n",
    "\n",
    "    Args:\n",
    "      proposal_feature_maps: A 4-D float tensor with shape\n",
    "        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n",
    "        representing the feature map cropped to each proposal.\n",
    "      scope: A scope name (unused).\n",
    "\n",
    "    Returns:\n",
    "      proposal_classifier_features: A 4-D float tensor with shape\n",
    "        [batch_size * self.max_num_proposals, height, width, depth]\n",
    "        representing box classifier features for each proposal.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(self._architecture, reuse=self._reuse_weights):\n",
    "      with slim.arg_scope(\n",
    "          resnet_utils.resnet_arg_scope(\n",
    "              batch_norm_epsilon=1e-5,\n",
    "              batch_norm_scale=True,\n",
    "              activation_fn=self._activation_fn,\n",
    "              weight_decay=self._weight_decay)):\n",
    "        with slim.arg_scope([slim.batch_norm],\n",
    "                            is_training=self._train_batch_norm):\n",
    "          blocks = [\n",
    "              resnet_utils.Block('block3', resnet_v1.bottleneck, [{\n",
    "                  'depth': 2048,\n",
    "                  'depth_bottleneck': 256,\n",
    "                  'stride': 2\n",
    "              }] * 6),\n",
    "              resnet_utils.Block('block4', resnet_v1.bottleneck, [{\n",
    "                  'depth': 2048,\n",
    "                  'depth_bottleneck': 512,\n",
    "                  'stride': 1\n",
    "              }] * 3)\n",
    "          ]\n",
    "          proposal_classifier_features = resnet_utils.stack_blocks_dense(\n",
    "              proposal_feature_maps, blocks)\n",
    "    return proposal_classifier_features\n",
    "\n",
    "\n",
    "\n",
    "class FasterRCNNResnet50FeatureExtractor(FasterRCNNResnetV1FeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet 50 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16,\n",
    "        or if `architecture` is not supported.\n",
    "    \"\"\"\n",
    "    super(FasterRCNNResnet50FeatureExtractor, self).__init__(\n",
    "        'resnet_v1_50', resnet_v1.resnet_v1_50, is_training,\n",
    "        first_stage_features_stride, batch_norm_trainable,\n",
    "        reuse_weights, weight_decay)\n",
    "\n",
    "\n",
    "class FasterRCNNResnet101FeatureExtractor(FasterRCNNResnetV1FeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet 101 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16,\n",
    "        or if `architecture` is not supported.\n",
    "    \"\"\"\n",
    "    super(FasterRCNNResnet101FeatureExtractor, self).__init__(\n",
    "        'resnet_v1_101', resnet_v1.resnet_v1_101, is_training,\n",
    "        first_stage_features_stride, batch_norm_trainable,\n",
    "        reuse_weights, weight_decay)\n",
    "\n",
    "\n",
    "class FasterRCNNResnet152FeatureExtractor(FasterRCNNResnetV1FeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet 152 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16,\n",
    "        or if `architecture` is not supported.\n",
    "    \"\"\"\n",
    "    super(FasterRCNNResnet152FeatureExtractor, self).__init__(\n",
    "        'resnet_v1_152', resnet_v1.resnet_v1_152, is_training,\n",
    "        first_stage_features_stride, batch_norm_trainable,\n",
    "        reuse_weights, weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djPtppTqZUnu"
   },
   "source": [
    "# load the new feature extractor in model_builder.py and write to models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cwkU4fdI0za_",
    "outputId": "c1d07e47-8fc0-4bd7-898f-3a64ff0a88c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/models/research/object_detection/builders/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/models/research/object_detection/builders/model_builder.py\n",
    "\n",
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A function to build a DetectionModel from configuration.\"\"\"\n",
    "\n",
    "import functools\n",
    "\n",
    "from object_detection.builders import anchor_generator_builder\n",
    "from object_detection.builders import box_coder_builder\n",
    "from object_detection.builders import box_predictor_builder\n",
    "from object_detection.builders import hyperparams_builder\n",
    "from object_detection.builders import image_resizer_builder\n",
    "from object_detection.builders import losses_builder\n",
    "from object_detection.builders import matcher_builder\n",
    "from object_detection.builders import post_processing_builder\n",
    "from object_detection.builders import region_similarity_calculator_builder as sim_calc\n",
    "from object_detection.core import balanced_positive_negative_sampler as sampler\n",
    "from object_detection.core import post_processing\n",
    "from object_detection.core import target_assigner\n",
    "from object_detection.meta_architectures import faster_rcnn_meta_arch\n",
    "from object_detection.meta_architectures import rfcn_meta_arch\n",
    "from object_detection.meta_architectures import ssd_meta_arch\n",
    "from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res\n",
    "from object_detection.models import faster_rcnn_inception_resnet_v2_keras_feature_extractor as frcnn_inc_res_keras\n",
    "from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2\n",
    "from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas\n",
    "from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas\n",
    "from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1\n",
    "from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn\n",
    "from object_detection.models import ssd_resnet_v1_fpn_keras_feature_extractor as ssd_resnet_v1_fpn_keras\n",
    "from object_detection.models import ssd_resnet_v1_ppn_feature_extractor as ssd_resnet_v1_ppn\n",
    "from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor\n",
    "from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor\n",
    "from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_edgetpu_feature_extractor import SSDMobileNetEdgeTPUFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v1_fpn_keras_feature_extractor import SSDMobileNetV1FpnKerasFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v1_keras_feature_extractor import SSDMobileNetV1KerasFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v2_fpn_feature_extractor import SSDMobileNetV2FpnFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v2_fpn_keras_feature_extractor import SSDMobileNetV2FpnKerasFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v2_keras_feature_extractor import SSDMobileNetV2KerasFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3LargeFeatureExtractor\n",
    "from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3SmallFeatureExtractor\n",
    "from object_detection.models.ssd_pnasnet_feature_extractor import SSDPNASNetFeatureExtractor\n",
    "from object_detection.predictors import rfcn_box_predictor\n",
    "from object_detection.predictors import rfcn_keras_box_predictor\n",
    "from object_detection.predictors.heads import mask_head\n",
    "from object_detection.protos import model_pb2\n",
    "from object_detection.utils import ops\n",
    "\n",
    "from object_detection.models import faster_rcnn_resnet_v100_feature_extractor as frcnn_resnet_v100\n",
    "\n",
    "# A map of names to SSD feature extractors.\n",
    "SSD_FEATURE_EXTRACTOR_CLASS_MAP = {\n",
    "    'ssd_inception_v2': SSDInceptionV2FeatureExtractor,\n",
    "    'ssd_inception_v3': SSDInceptionV3FeatureExtractor,\n",
    "    'ssd_mobilenet_v1': SSDMobileNetV1FeatureExtractor,\n",
    "    'ssd_mobilenet_v1_fpn': SSDMobileNetV1FpnFeatureExtractor,\n",
    "    'ssd_mobilenet_v1_ppn': SSDMobileNetV1PpnFeatureExtractor,\n",
    "    'ssd_mobilenet_v2': SSDMobileNetV2FeatureExtractor,\n",
    "    'ssd_mobilenet_v2_fpn': SSDMobileNetV2FpnFeatureExtractor,\n",
    "    'ssd_mobilenet_v3_large': SSDMobileNetV3LargeFeatureExtractor,\n",
    "    'ssd_mobilenet_v3_small': SSDMobileNetV3SmallFeatureExtractor,\n",
    "    'ssd_mobilenet_edgetpu': SSDMobileNetEdgeTPUFeatureExtractor,\n",
    "    'ssd_resnet50_v1_fpn': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,\n",
    "    'ssd_resnet101_v1_fpn': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,\n",
    "    'ssd_resnet152_v1_fpn': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,\n",
    "    'ssd_resnet50_v1_ppn': ssd_resnet_v1_ppn.SSDResnet50V1PpnFeatureExtractor,\n",
    "    'ssd_resnet101_v1_ppn':\n",
    "        ssd_resnet_v1_ppn.SSDResnet101V1PpnFeatureExtractor,\n",
    "    'ssd_resnet152_v1_ppn':\n",
    "        ssd_resnet_v1_ppn.SSDResnet152V1PpnFeatureExtractor,\n",
    "    'embedded_ssd_mobilenet_v1': EmbeddedSSDMobileNetV1FeatureExtractor,\n",
    "    'ssd_pnasnet': SSDPNASNetFeatureExtractor,\n",
    "}\n",
    "\n",
    "SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {\n",
    "    'ssd_mobilenet_v1_keras': SSDMobileNetV1KerasFeatureExtractor,\n",
    "    'ssd_mobilenet_v1_fpn_keras': SSDMobileNetV1FpnKerasFeatureExtractor,\n",
    "    'ssd_mobilenet_v2_keras': SSDMobileNetV2KerasFeatureExtractor,\n",
    "    'ssd_mobilenet_v2_fpn_keras': SSDMobileNetV2FpnKerasFeatureExtractor,\n",
    "    'ssd_resnet50_v1_fpn_keras':\n",
    "        ssd_resnet_v1_fpn_keras.SSDResNet50V1FpnKerasFeatureExtractor,\n",
    "    'ssd_resnet101_v1_fpn_keras':\n",
    "        ssd_resnet_v1_fpn_keras.SSDResNet101V1FpnKerasFeatureExtractor,\n",
    "    'ssd_resnet152_v1_fpn_keras':\n",
    "        ssd_resnet_v1_fpn_keras.SSDResNet152V1FpnKerasFeatureExtractor,\n",
    "}\n",
    "\n",
    "# A map of names to Faster R-CNN feature extractors.\n",
    "FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {\n",
    "    'faster_rcnn_nas':\n",
    "    frcnn_nas.FasterRCNNNASFeatureExtractor,\n",
    "    'faster_rcnn_pnas':\n",
    "    frcnn_pnas.FasterRCNNPNASFeatureExtractor,\n",
    "    'faster_rcnn_inception_resnet_v2':\n",
    "    frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor,\n",
    "    'faster_rcnn_inception_v2':\n",
    "    frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor,\n",
    "    'faster_rcnn_resnet50':\n",
    "    frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,\n",
    "    'faster_rcnn_resnet101':\n",
    "    frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,\n",
    "    'faster_rcnn_resnet152':\n",
    "    frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor,\n",
    "\n",
    "    'faster_rcnn_resnet_v100':\n",
    "    frcnn_resnet_v100.FasterRCNNResnet50FeatureExtractor,\n",
    "}\n",
    "\n",
    "FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {\n",
    "    'faster_rcnn_inception_resnet_v2_keras':\n",
    "    frcnn_inc_res_keras.FasterRCNNInceptionResnetV2KerasFeatureExtractor,\n",
    "}\n",
    "\n",
    "\n",
    "def _build_ssd_feature_extractor(feature_extractor_config,\n",
    "                                 is_training,\n",
    "                                 freeze_batchnorm,\n",
    "                                 reuse_weights=None):\n",
    "  \"\"\"Builds a ssd_meta_arch.SSDFeatureExtractor based on config.\n",
    "\n",
    "  Args:\n",
    "    feature_extractor_config: A SSDFeatureExtractor proto config from ssd.proto.\n",
    "    is_training: True if this feature extractor is being built for training.\n",
    "    freeze_batchnorm: Whether to freeze batch norm parameters during\n",
    "      training or not. When training with a small batch size (e.g. 1), it is\n",
    "      desirable to freeze batch norm update and use pretrained batch norm\n",
    "      params.\n",
    "    reuse_weights: if the feature extractor should reuse weights.\n",
    "\n",
    "  Returns:\n",
    "    ssd_meta_arch.SSDFeatureExtractor based on config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid feature extractor type.\n",
    "  \"\"\"\n",
    "  feature_type = feature_extractor_config.type\n",
    "  is_keras_extractor = feature_type in SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP\n",
    "  depth_multiplier = feature_extractor_config.depth_multiplier\n",
    "  min_depth = feature_extractor_config.min_depth\n",
    "  pad_to_multiple = feature_extractor_config.pad_to_multiple\n",
    "  use_explicit_padding = feature_extractor_config.use_explicit_padding\n",
    "  use_depthwise = feature_extractor_config.use_depthwise\n",
    "\n",
    "  if is_keras_extractor:\n",
    "    conv_hyperparams = hyperparams_builder.KerasLayerHyperparams(\n",
    "        feature_extractor_config.conv_hyperparams)\n",
    "  else:\n",
    "    conv_hyperparams = hyperparams_builder.build(\n",
    "        feature_extractor_config.conv_hyperparams, is_training)\n",
    "  override_base_feature_extractor_hyperparams = (\n",
    "      feature_extractor_config.override_base_feature_extractor_hyperparams)\n",
    "\n",
    "  if (feature_type not in SSD_FEATURE_EXTRACTOR_CLASS_MAP) and (\n",
    "      not is_keras_extractor):\n",
    "    raise ValueError('Unknown ssd feature_extractor: {}'.format(feature_type))\n",
    "\n",
    "  if is_keras_extractor:\n",
    "    feature_extractor_class = SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP[\n",
    "        feature_type]\n",
    "  else:\n",
    "    feature_extractor_class = SSD_FEATURE_EXTRACTOR_CLASS_MAP[feature_type]\n",
    "  kwargs = {\n",
    "      'is_training':\n",
    "          is_training,\n",
    "      'depth_multiplier':\n",
    "          depth_multiplier,\n",
    "      'min_depth':\n",
    "          min_depth,\n",
    "      'pad_to_multiple':\n",
    "          pad_to_multiple,\n",
    "      'use_explicit_padding':\n",
    "          use_explicit_padding,\n",
    "      'use_depthwise':\n",
    "          use_depthwise,\n",
    "      'override_base_feature_extractor_hyperparams':\n",
    "          override_base_feature_extractor_hyperparams\n",
    "  }\n",
    "\n",
    "  if feature_extractor_config.HasField('replace_preprocessor_with_placeholder'):\n",
    "    kwargs.update({\n",
    "        'replace_preprocessor_with_placeholder':\n",
    "            feature_extractor_config.replace_preprocessor_with_placeholder\n",
    "    })\n",
    "\n",
    "  if feature_extractor_config.HasField('num_layers'):\n",
    "    kwargs.update({'num_layers': feature_extractor_config.num_layers})\n",
    "\n",
    "  if is_keras_extractor:\n",
    "    kwargs.update({\n",
    "        'conv_hyperparams': conv_hyperparams,\n",
    "        'inplace_batchnorm_update': False,\n",
    "        'freeze_batchnorm': freeze_batchnorm\n",
    "    })\n",
    "  else:\n",
    "    kwargs.update({\n",
    "        'conv_hyperparams_fn': conv_hyperparams,\n",
    "        'reuse_weights': reuse_weights,\n",
    "    })\n",
    "\n",
    "  if feature_extractor_config.HasField('fpn'):\n",
    "    kwargs.update({\n",
    "        'fpn_min_level':\n",
    "            feature_extractor_config.fpn.min_level,\n",
    "        'fpn_max_level':\n",
    "            feature_extractor_config.fpn.max_level,\n",
    "        'additional_layer_depth':\n",
    "            feature_extractor_config.fpn.additional_layer_depth,\n",
    "    })\n",
    "\n",
    "\n",
    "  return feature_extractor_class(**kwargs)\n",
    "\n",
    "\n",
    "def _build_ssd_model(ssd_config, is_training, add_summaries):\n",
    "  \"\"\"Builds an SSD detection model based on the model config.\n",
    "\n",
    "  Args:\n",
    "    ssd_config: A ssd.proto object containing the config for the desired\n",
    "      SSDMetaArch.\n",
    "    is_training: True if this model is being built for training purposes.\n",
    "    add_summaries: Whether to add tf summaries in the model.\n",
    "  Returns:\n",
    "    SSDMetaArch based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If ssd_config.type is not recognized (i.e. not registered in\n",
    "      model_class_map).\n",
    "  \"\"\"\n",
    "  num_classes = ssd_config.num_classes\n",
    "\n",
    "  # Feature extractor\n",
    "  feature_extractor = _build_ssd_feature_extractor(\n",
    "      feature_extractor_config=ssd_config.feature_extractor,\n",
    "      freeze_batchnorm=ssd_config.freeze_batchnorm,\n",
    "      is_training=is_training)\n",
    "\n",
    "  box_coder = box_coder_builder.build(ssd_config.box_coder)\n",
    "  matcher = matcher_builder.build(ssd_config.matcher)\n",
    "  region_similarity_calculator = sim_calc.build(\n",
    "      ssd_config.similarity_calculator)\n",
    "  encode_background_as_zeros = ssd_config.encode_background_as_zeros\n",
    "  negative_class_weight = ssd_config.negative_class_weight\n",
    "  anchor_generator = anchor_generator_builder.build(\n",
    "      ssd_config.anchor_generator)\n",
    "  if feature_extractor.is_keras_model:\n",
    "    ssd_box_predictor = box_predictor_builder.build_keras(\n",
    "        hyperparams_fn=hyperparams_builder.KerasLayerHyperparams,\n",
    "        freeze_batchnorm=ssd_config.freeze_batchnorm,\n",
    "        inplace_batchnorm_update=False,\n",
    "        num_predictions_per_location_list=anchor_generator\n",
    "        .num_anchors_per_location(),\n",
    "        box_predictor_config=ssd_config.box_predictor,\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        add_background_class=ssd_config.add_background_class)\n",
    "  else:\n",
    "    ssd_box_predictor = box_predictor_builder.build(\n",
    "        hyperparams_builder.build, ssd_config.box_predictor, is_training,\n",
    "        num_classes, ssd_config.add_background_class)\n",
    "  image_resizer_fn = image_resizer_builder.build(ssd_config.image_resizer)\n",
    "  non_max_suppression_fn, score_conversion_fn = post_processing_builder.build(\n",
    "      ssd_config.post_processing)\n",
    "  (classification_loss, localization_loss, classification_weight,\n",
    "   localization_weight, hard_example_miner, random_example_sampler,\n",
    "   expected_loss_weights_fn) = losses_builder.build(ssd_config.loss)\n",
    "  normalize_loss_by_num_matches = ssd_config.normalize_loss_by_num_matches\n",
    "  normalize_loc_loss_by_codesize = ssd_config.normalize_loc_loss_by_codesize\n",
    "\n",
    "  equalization_loss_config = ops.EqualizationLossConfig(\n",
    "      weight=ssd_config.loss.equalization_loss.weight,\n",
    "      exclude_prefixes=ssd_config.loss.equalization_loss.exclude_prefixes)\n",
    "\n",
    "  target_assigner_instance = target_assigner.TargetAssigner(\n",
    "      region_similarity_calculator,\n",
    "      matcher,\n",
    "      box_coder,\n",
    "      negative_class_weight=negative_class_weight)\n",
    "\n",
    "  ssd_meta_arch_fn = ssd_meta_arch.SSDMetaArch\n",
    "  kwargs = {}\n",
    "\n",
    "  return ssd_meta_arch_fn(\n",
    "      is_training=is_training,\n",
    "      anchor_generator=anchor_generator,\n",
    "      box_predictor=ssd_box_predictor,\n",
    "      box_coder=box_coder,\n",
    "      feature_extractor=feature_extractor,\n",
    "      encode_background_as_zeros=encode_background_as_zeros,\n",
    "      image_resizer_fn=image_resizer_fn,\n",
    "      non_max_suppression_fn=non_max_suppression_fn,\n",
    "      score_conversion_fn=score_conversion_fn,\n",
    "      classification_loss=classification_loss,\n",
    "      localization_loss=localization_loss,\n",
    "      classification_loss_weight=classification_weight,\n",
    "      localization_loss_weight=localization_weight,\n",
    "      normalize_loss_by_num_matches=normalize_loss_by_num_matches,\n",
    "      hard_example_miner=hard_example_miner,\n",
    "      target_assigner_instance=target_assigner_instance,\n",
    "      add_summaries=add_summaries,\n",
    "      normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize,\n",
    "      freeze_batchnorm=ssd_config.freeze_batchnorm,\n",
    "      inplace_batchnorm_update=ssd_config.inplace_batchnorm_update,\n",
    "      add_background_class=ssd_config.add_background_class,\n",
    "      explicit_background_class=ssd_config.explicit_background_class,\n",
    "      random_example_sampler=random_example_sampler,\n",
    "      expected_loss_weights_fn=expected_loss_weights_fn,\n",
    "      use_confidences_as_targets=ssd_config.use_confidences_as_targets,\n",
    "      implicit_example_weight=ssd_config.implicit_example_weight,\n",
    "      equalization_loss_config=equalization_loss_config,\n",
    "      return_raw_detections_during_predict=(\n",
    "          ssd_config.return_raw_detections_during_predict),\n",
    "      **kwargs)\n",
    "\n",
    "\n",
    "def _build_faster_rcnn_feature_extractor(\n",
    "    feature_extractor_config, is_training, reuse_weights=None,\n",
    "    inplace_batchnorm_update=False):\n",
    "  \"\"\"Builds a faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.\n",
    "\n",
    "  Args:\n",
    "    feature_extractor_config: A FasterRcnnFeatureExtractor proto config from\n",
    "      faster_rcnn.proto.\n",
    "    is_training: True if this feature extractor is being built for training.\n",
    "    reuse_weights: if the feature extractor should reuse weights.\n",
    "    inplace_batchnorm_update: Whether to update batch_norm inplace during\n",
    "      training. This is required for batch norm to work correctly on TPUs. When\n",
    "      this is false, user must add a control dependency on\n",
    "      tf.GraphKeys.UPDATE_OPS for train/loss op in order to update the batch\n",
    "      norm moving average parameters.\n",
    "\n",
    "  Returns:\n",
    "    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid feature extractor type.\n",
    "  \"\"\"\n",
    "  if inplace_batchnorm_update:\n",
    "    raise ValueError('inplace batchnorm updates not supported.')\n",
    "  feature_type = feature_extractor_config.type\n",
    "  first_stage_features_stride = (\n",
    "      feature_extractor_config.first_stage_features_stride)\n",
    "  batch_norm_trainable = feature_extractor_config.batch_norm_trainable\n",
    "\n",
    "  if feature_type not in FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP:\n",
    "    raise ValueError('Unknown Faster R-CNN feature_extractor: {}'.format(\n",
    "        feature_type))\n",
    "  feature_extractor_class = FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP[\n",
    "      feature_type]\n",
    "  return feature_extractor_class(\n",
    "      is_training, first_stage_features_stride,\n",
    "      batch_norm_trainable, reuse_weights=reuse_weights)\n",
    "\n",
    "\n",
    "def _build_faster_rcnn_keras_feature_extractor(\n",
    "    feature_extractor_config, is_training,\n",
    "    inplace_batchnorm_update=False):\n",
    "  \"\"\"Builds a faster_rcnn_meta_arch.FasterRCNNKerasFeatureExtractor from config.\n",
    "\n",
    "  Args:\n",
    "    feature_extractor_config: A FasterRcnnFeatureExtractor proto config from\n",
    "      faster_rcnn.proto.\n",
    "    is_training: True if this feature extractor is being built for training.\n",
    "    inplace_batchnorm_update: Whether to update batch_norm inplace during\n",
    "      training. This is required for batch norm to work correctly on TPUs. When\n",
    "      this is false, user must add a control dependency on\n",
    "      tf.GraphKeys.UPDATE_OPS for train/loss op in order to update the batch\n",
    "      norm moving average parameters.\n",
    "\n",
    "  Returns:\n",
    "    faster_rcnn_meta_arch.FasterRCNNKerasFeatureExtractor based on config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid feature extractor type.\n",
    "  \"\"\"\n",
    "  if inplace_batchnorm_update:\n",
    "    raise ValueError('inplace batchnorm updates not supported.')\n",
    "  feature_type = feature_extractor_config.type\n",
    "  first_stage_features_stride = (\n",
    "      feature_extractor_config.first_stage_features_stride)\n",
    "  batch_norm_trainable = feature_extractor_config.batch_norm_trainable\n",
    "\n",
    "  if feature_type not in FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP:\n",
    "    raise ValueError('Unknown Faster R-CNN feature_extractor: {}'.format(\n",
    "        feature_type))\n",
    "  feature_extractor_class = FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP[\n",
    "      feature_type]\n",
    "  return feature_extractor_class(\n",
    "      is_training, first_stage_features_stride,\n",
    "      batch_norm_trainable)\n",
    "\n",
    "\n",
    "def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):\n",
    "  \"\"\"Builds a Faster R-CNN or R-FCN detection model based on the model config.\n",
    "\n",
    "  Builds R-FCN model if the second_stage_box_predictor in the config is of type\n",
    "  `rfcn_box_predictor` else builds a Faster R-CNN model.\n",
    "\n",
    "  Args:\n",
    "    frcnn_config: A faster_rcnn.proto object containing the config for the\n",
    "      desired FasterRCNNMetaArch or RFCNMetaArch.\n",
    "    is_training: True if this model is being built for training purposes.\n",
    "    add_summaries: Whether to add tf summaries in the model.\n",
    "\n",
    "  Returns:\n",
    "    FasterRCNNMetaArch based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If frcnn_config.type is not recognized (i.e. not registered in\n",
    "      model_class_map).\n",
    "  \"\"\"\n",
    "  num_classes = frcnn_config.num_classes\n",
    "  image_resizer_fn = image_resizer_builder.build(frcnn_config.image_resizer)\n",
    "\n",
    "  is_keras = (frcnn_config.feature_extractor.type in\n",
    "              FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP)\n",
    "\n",
    "  if is_keras:\n",
    "    feature_extractor = _build_faster_rcnn_keras_feature_extractor(\n",
    "        frcnn_config.feature_extractor, is_training,\n",
    "        inplace_batchnorm_update=frcnn_config.inplace_batchnorm_update)\n",
    "  else:\n",
    "    feature_extractor = _build_faster_rcnn_feature_extractor(\n",
    "        frcnn_config.feature_extractor, is_training,\n",
    "        inplace_batchnorm_update=frcnn_config.inplace_batchnorm_update)\n",
    "\n",
    "  number_of_stages = frcnn_config.number_of_stages\n",
    "  first_stage_anchor_generator = anchor_generator_builder.build(\n",
    "      frcnn_config.first_stage_anchor_generator)\n",
    "\n",
    "  first_stage_target_assigner = target_assigner.create_target_assigner(\n",
    "      'FasterRCNN',\n",
    "      'proposal',\n",
    "      use_matmul_gather=frcnn_config.use_matmul_gather_in_matcher)\n",
    "  first_stage_atrous_rate = frcnn_config.first_stage_atrous_rate\n",
    "  if is_keras:\n",
    "    first_stage_box_predictor_arg_scope_fn = (\n",
    "        hyperparams_builder.KerasLayerHyperparams(\n",
    "            frcnn_config.first_stage_box_predictor_conv_hyperparams))\n",
    "  else:\n",
    "    first_stage_box_predictor_arg_scope_fn = hyperparams_builder.build(\n",
    "        frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)\n",
    "  first_stage_box_predictor_kernel_size = (\n",
    "      frcnn_config.first_stage_box_predictor_kernel_size)\n",
    "  first_stage_box_predictor_depth = frcnn_config.first_stage_box_predictor_depth\n",
    "  first_stage_minibatch_size = frcnn_config.first_stage_minibatch_size\n",
    "  use_static_shapes = frcnn_config.use_static_shapes and (\n",
    "      frcnn_config.use_static_shapes_for_eval or is_training)\n",
    "  first_stage_sampler = sampler.BalancedPositiveNegativeSampler(\n",
    "      positive_fraction=frcnn_config.first_stage_positive_balance_fraction,\n",
    "      is_static=(frcnn_config.use_static_balanced_label_sampler and\n",
    "                 use_static_shapes))\n",
    "  first_stage_max_proposals = frcnn_config.first_stage_max_proposals\n",
    "  if (frcnn_config.first_stage_nms_iou_threshold < 0 or\n",
    "      frcnn_config.first_stage_nms_iou_threshold > 1.0):\n",
    "    raise ValueError('iou_threshold not in [0, 1.0].')\n",
    "  if (is_training and frcnn_config.second_stage_batch_size >\n",
    "      first_stage_max_proposals):\n",
    "    raise ValueError('second_stage_batch_size should be no greater than '\n",
    "                     'first_stage_max_proposals.')\n",
    "  first_stage_non_max_suppression_fn = functools.partial(\n",
    "      post_processing.batch_multiclass_non_max_suppression,\n",
    "      score_thresh=frcnn_config.first_stage_nms_score_threshold,\n",
    "      iou_thresh=frcnn_config.first_stage_nms_iou_threshold,\n",
    "      max_size_per_class=frcnn_config.first_stage_max_proposals,\n",
    "      max_total_size=frcnn_config.first_stage_max_proposals,\n",
    "      use_static_shapes=use_static_shapes,\n",
    "      use_partitioned_nms=frcnn_config.use_partitioned_nms_in_first_stage,\n",
    "      use_combined_nms=frcnn_config.use_combined_nms_in_first_stage)\n",
    "  first_stage_loc_loss_weight = (\n",
    "      frcnn_config.first_stage_localization_loss_weight)\n",
    "  first_stage_obj_loss_weight = frcnn_config.first_stage_objectness_loss_weight\n",
    "\n",
    "  initial_crop_size = frcnn_config.initial_crop_size\n",
    "  maxpool_kernel_size = frcnn_config.maxpool_kernel_size\n",
    "  maxpool_stride = frcnn_config.maxpool_stride\n",
    "\n",
    "  second_stage_target_assigner = target_assigner.create_target_assigner(\n",
    "      'FasterRCNN',\n",
    "      'detection',\n",
    "      use_matmul_gather=frcnn_config.use_matmul_gather_in_matcher)\n",
    "  if is_keras:\n",
    "    second_stage_box_predictor = box_predictor_builder.build_keras(\n",
    "        hyperparams_builder.KerasLayerHyperparams,\n",
    "        freeze_batchnorm=False,\n",
    "        inplace_batchnorm_update=False,\n",
    "        num_predictions_per_location_list=[1],\n",
    "        box_predictor_config=frcnn_config.second_stage_box_predictor,\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes)\n",
    "  else:\n",
    "    second_stage_box_predictor = box_predictor_builder.build(\n",
    "        hyperparams_builder.build,\n",
    "        frcnn_config.second_stage_box_predictor,\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes)\n",
    "  second_stage_batch_size = frcnn_config.second_stage_batch_size\n",
    "  second_stage_sampler = sampler.BalancedPositiveNegativeSampler(\n",
    "      positive_fraction=frcnn_config.second_stage_balance_fraction,\n",
    "      is_static=(frcnn_config.use_static_balanced_label_sampler and\n",
    "                 use_static_shapes))\n",
    "  (second_stage_non_max_suppression_fn, second_stage_score_conversion_fn\n",
    "  ) = post_processing_builder.build(frcnn_config.second_stage_post_processing)\n",
    "  second_stage_localization_loss_weight = (\n",
    "      frcnn_config.second_stage_localization_loss_weight)\n",
    "  second_stage_classification_loss = (\n",
    "      losses_builder.build_faster_rcnn_classification_loss(\n",
    "          frcnn_config.second_stage_classification_loss))\n",
    "  second_stage_classification_loss_weight = (\n",
    "      frcnn_config.second_stage_classification_loss_weight)\n",
    "  second_stage_mask_prediction_loss_weight = (\n",
    "      frcnn_config.second_stage_mask_prediction_loss_weight)\n",
    "\n",
    "  hard_example_miner = None\n",
    "  if frcnn_config.HasField('hard_example_miner'):\n",
    "    hard_example_miner = losses_builder.build_hard_example_miner(\n",
    "        frcnn_config.hard_example_miner,\n",
    "        second_stage_classification_loss_weight,\n",
    "        second_stage_localization_loss_weight)\n",
    "\n",
    "  crop_and_resize_fn = (\n",
    "      ops.matmul_crop_and_resize if frcnn_config.use_matmul_crop_and_resize\n",
    "      else ops.native_crop_and_resize)\n",
    "  clip_anchors_to_image = (\n",
    "      frcnn_config.clip_anchors_to_image)\n",
    "\n",
    "  common_kwargs = {\n",
    "      'is_training': is_training,\n",
    "      'num_classes': num_classes,\n",
    "      'image_resizer_fn': image_resizer_fn,\n",
    "      'feature_extractor': feature_extractor,\n",
    "      'number_of_stages': number_of_stages,\n",
    "      'first_stage_anchor_generator': first_stage_anchor_generator,\n",
    "      'first_stage_target_assigner': first_stage_target_assigner,\n",
    "      'first_stage_atrous_rate': first_stage_atrous_rate,\n",
    "      'first_stage_box_predictor_arg_scope_fn':\n",
    "      first_stage_box_predictor_arg_scope_fn,\n",
    "      'first_stage_box_predictor_kernel_size':\n",
    "      first_stage_box_predictor_kernel_size,\n",
    "      'first_stage_box_predictor_depth': first_stage_box_predictor_depth,\n",
    "      'first_stage_minibatch_size': first_stage_minibatch_size,\n",
    "      'first_stage_sampler': first_stage_sampler,\n",
    "      'first_stage_non_max_suppression_fn': first_stage_non_max_suppression_fn,\n",
    "      'first_stage_max_proposals': first_stage_max_proposals,\n",
    "      'first_stage_localization_loss_weight': first_stage_loc_loss_weight,\n",
    "      'first_stage_objectness_loss_weight': first_stage_obj_loss_weight,\n",
    "      'second_stage_target_assigner': second_stage_target_assigner,\n",
    "      'second_stage_batch_size': second_stage_batch_size,\n",
    "      'second_stage_sampler': second_stage_sampler,\n",
    "      'second_stage_non_max_suppression_fn':\n",
    "      second_stage_non_max_suppression_fn,\n",
    "      'second_stage_score_conversion_fn': second_stage_score_conversion_fn,\n",
    "      'second_stage_localization_loss_weight':\n",
    "      second_stage_localization_loss_weight,\n",
    "      'second_stage_classification_loss':\n",
    "      second_stage_classification_loss,\n",
    "      'second_stage_classification_loss_weight':\n",
    "      second_stage_classification_loss_weight,\n",
    "      'hard_example_miner': hard_example_miner,\n",
    "      'add_summaries': add_summaries,\n",
    "      'crop_and_resize_fn': crop_and_resize_fn,\n",
    "      'clip_anchors_to_image': clip_anchors_to_image,\n",
    "      'use_static_shapes': use_static_shapes,\n",
    "      'resize_masks': frcnn_config.resize_masks,\n",
    "      'return_raw_detections_during_predict': (\n",
    "          frcnn_config.return_raw_detections_during_predict)\n",
    "  }\n",
    "\n",
    "  if (isinstance(second_stage_box_predictor,\n",
    "                 rfcn_box_predictor.RfcnBoxPredictor) or\n",
    "      isinstance(second_stage_box_predictor,\n",
    "                 rfcn_keras_box_predictor.RfcnKerasBoxPredictor)):\n",
    "    return rfcn_meta_arch.RFCNMetaArch(\n",
    "        second_stage_rfcn_box_predictor=second_stage_box_predictor,\n",
    "        **common_kwargs)\n",
    "  else:\n",
    "    return faster_rcnn_meta_arch.FasterRCNNMetaArch(\n",
    "        initial_crop_size=initial_crop_size,\n",
    "        maxpool_kernel_size=maxpool_kernel_size,\n",
    "        maxpool_stride=maxpool_stride,\n",
    "        second_stage_mask_rcnn_box_predictor=second_stage_box_predictor,\n",
    "        second_stage_mask_prediction_loss_weight=(\n",
    "            second_stage_mask_prediction_loss_weight),\n",
    "        **common_kwargs)\n",
    "\n",
    "EXPERIMENTAL_META_ARCH_BUILDER_MAP = {\n",
    "}\n",
    "\n",
    "\n",
    "def _build_experimental_model(config, is_training, add_summaries=True):\n",
    "  return EXPERIMENTAL_META_ARCH_BUILDER_MAP[config.name](\n",
    "      is_training, add_summaries)\n",
    "\n",
    "META_ARCHITECURE_BUILDER_MAP = {\n",
    "    'ssd': _build_ssd_model,\n",
    "    'faster_rcnn': _build_faster_rcnn_model,\n",
    "    'experimental_model': _build_experimental_model\n",
    "}\n",
    "\n",
    "\n",
    "def build(model_config, is_training, add_summaries=True):\n",
    "  \"\"\"Builds a DetectionModel based on the model config.\n",
    "\n",
    "  Args:\n",
    "    model_config: A model.proto object containing the config for the desired\n",
    "      DetectionModel.\n",
    "    is_training: True if this model is being built for training purposes.\n",
    "    add_summaries: Whether to add tensorflow summaries in the model graph.\n",
    "  Returns:\n",
    "    DetectionModel based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid meta architecture or model.\n",
    "  \"\"\"\n",
    "  if not isinstance(model_config, model_pb2.DetectionModel):\n",
    "    raise ValueError('model_config not of type model_pb2.DetectionModel.')\n",
    "\n",
    "  meta_architecture = model_config.WhichOneof('model')\n",
    "\n",
    "  if meta_architecture not in META_ARCHITECURE_BUILDER_MAP:\n",
    "    raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))\n",
    "  else:\n",
    "    build_func = META_ARCHITECURE_BUILDER_MAP[meta_architecture]\n",
    "    return build_func(getattr(model_config, meta_architecture), is_training,\n",
    "                      add_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8XwRTO2a1Vz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "actual inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
